services:
  llama-vision:
    build:
      context: .
      dockerfile: Dockerfile
    image: llama-vision:latest
    container_name: llama-vision
    ports:
      - "5000:5000"
    volumes:
      # Mount your models directory here
      - ./models:/models
    environment:
      # Configure your model names
      - MODEL_NAME=llama-3.2-11b-vision-instruct-q4_k_m.gguf
      - CLIP_MODEL_NAME=mmproj-model-f16.gguf
      - MODEL_PATH=/models
      - PORT=5000
    restart: unless-stopped
    # Resource limits (adjust based on your needs)
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
        reservations:
          cpus: '2'
          memory: 4G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
